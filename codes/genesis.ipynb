{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    Prepare spatial data for analysis by performing several preprocessing steps.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing spatial data, including a 'geometry' column.\n",
    "\n",
    "    Returns:\n",
    "    - Processed DataFrame with additional features and unnecessary columns dropped.\n",
    "\n",
    "    Steps:\n",
    "    1. Drop unnecessary features like 'index' and 'geometry'.\n",
    "    2. Convert the 'geometry' column to the EPSG:32633 coordinate reference system.\n",
    "    3. Calculate the area of each geometry and add it as a new column 'area'.\n",
    "    4. Calculate the perimeter of each geometry and add it as a new column 'perimeter'.\n",
    "    5. Calculate the x-coordinate of the centroid of each geometry and add it as a new column 'centroid_x'.\n",
    "    6. Calculate the y-coordinate of the centroid of each geometry and add it as a new column 'centroid_y'.\n",
    "    7. Calculate the boundary length of each geometry and add it as a new column 'boundary_length'.\n",
    "    8. Drop the columns specified in 'drop_features' list.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define features to drop\n",
    "    drop_features = ['index', 'geometry']\n",
    "    \n",
    "    # Convert the 'geometry' column to EPSG:32633 coordinate reference system\n",
    "    data['geometry'] = data['geometry'].to_crs('EPSG:32633')\n",
    "    \n",
    "    # Calculate area of each geometry and add it as a new column 'area'\n",
    "    data['area'] = data[['geometry']].area\n",
    "    \n",
    "    # Calculate perimeter of each geometry and add it as a new column 'perimeter'\n",
    "    data['perimeter'] = data[['geometry']].length\n",
    "    \n",
    "    # Calculate x-coordinate of centroid of each geometry and add it as a new column 'centroid_x'\n",
    "    data['centroid_x'] = data[['geometry']].centroid.x\n",
    "    \n",
    "    # Calculate y-coordinate of centroid of each geometry and add it as a new column 'centroid_y'\n",
    "    data['centroid_y'] = data[['geometry']].centroid.y\n",
    "    \n",
    "    # Calculate boundary length of each geometry and add it as a new column 'boundary_length'\n",
    "    data['boundary_length'] = data[['geometry']].boundary.length\n",
    "    \n",
    "    # Drop unnecessary features\n",
    "    data = data.drop(columns=drop_features)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_train_data(data_filtered):\n",
    "    \"\"\"\n",
    "    Filter the training data by removing rows with too many NaN values and ensuring specified columns are not None.\n",
    "\n",
    "    Parameters:\n",
    "    - data_filtered: DataFrame containing the training data.\n",
    "\n",
    "    Returns:\n",
    "    - Filtered DataFrame with rows removed based on the specified criteria.\n",
    "\n",
    "    Steps:\n",
    "    1. Identify NaN values in the DataFrame.\n",
    "    2. Count the number of NaN values in each row.\n",
    "    3. Select rows with fewer than 5 NaN values and ensure specified columns are not None.\n",
    "    4. Filter the DataFrame to keep only the selected rows.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check for NaN values in the DataFrame\n",
    "    nan_values = data_filtered.isna()\n",
    "\n",
    "    # Count the number of NaN values in each row\n",
    "    nan_count_per_row = nan_values.sum(axis=1)\n",
    "\n",
    "    # Select rows with fewer than 5 NaN values and ensure specified columns are not None\n",
    "    index_maintain = data_filtered[(nan_count_per_row < 5) & (~data_filtered[['date0','date1','date2','date3','date4']].isna().any(axis=1))].index \n",
    "\n",
    "    # Filter the DataFrame to keep only the selected rows\n",
    "    data_filtered = data_filtered.iloc[index_maintain, :]\n",
    "\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan_cols(df):\n",
    "    \"\"\"\n",
    "    Count the number of missing values in each column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Steps:\n",
    "    1. Use isnull() to create a DataFrame of boolean values indicating missing values.\n",
    "    2. Use sum() to calculate the total number of missing values in each column.\n",
    "    3. Sort the result in descending order.\n",
    "    4. Print the columns with the most missing values.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use isnull() to create a DataFrame of boolean values indicating missing values\n",
    "    missing_values = df.isnull()\n",
    "\n",
    "    # Use sum() to calculate the total number of missing values in each column\n",
    "    missing_counts = missing_values.sum()\n",
    "\n",
    "    # Sort the result in descending order\n",
    "    missing_counts_sorted = missing_counts.sort_values(ascending=False)\n",
    "\n",
    "    # Print the columns with the most missing values\n",
    "    print(\"Columns with the most missing values:\")\n",
    "    print(missing_counts_sorted[missing_counts_sorted > 0])  # Adjust the number as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_imputer(dataframe):    \n",
    "    \"\"\"\n",
    "    Impute missing numerical values in a DataFrame using the IterativeImputer.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: DataFrame containing numerical columns with missing values.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with missing numerical values imputed.\n",
    "\n",
    "    Steps:\n",
    "    1. Select numerical columns from the DataFrame.\n",
    "    2. Initialize an IterativeImputer.\n",
    "    3. Fit the imputer on rows without missing numerical values.\n",
    "    4. Transform rows with missing numerical values using the imputer.\n",
    "    5. Return the DataFrame with missing numerical values imputed.\n",
    "\n",
    "    \"\"\"\n",
    "    numerical_columns_selector = selector(dtype_include=[int, float])\n",
    "    numerical_columns = numerical_columns_selector(dataframe)\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    # imputer = IterativeImputer()\n",
    "    imputer.fit(dataframe[dataframe[numerical_columns].notna().all(axis=1)])\n",
    "    dataframe[dataframe[numerical_columns].isna().any(axis=1)] = imputer.transform(dataframe[dataframe[numerical_columns].isna().any(axis=1)])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data):\n",
    "    \"\"\"\n",
    "    Create balanced data for training by oversampling with SMOTE.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing features and target variable.\n",
    "\n",
    "    Returns:\n",
    "    - X_data: Features of the balanced dataset.\n",
    "    - y_data: Target variable of the balanced dataset.\n",
    "\n",
    "    Steps:\n",
    "    1. Define a dictionary specifying the desired sampling strategy for each class.\n",
    "    2. Separate features and target variable from the DataFrame.\n",
    "    3. Initialize SMOTE with the specified sampling strategy.\n",
    "    4. Oversample the minority classes to balance the dataset.\n",
    "    5. Return the balanced features and target variable.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the desired sampling strategy for each class\n",
    "    strategy = {0:40000, 1:40000, 3:150000, 4:10000, 5:5000}\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X_data = data.drop(columns=['change_type'])\n",
    "    y_data = data['change_type']\n",
    "\n",
    "    # Initialize SMOTE with the specified sampling strategy\n",
    "    oversample = SMOTE(sampling_strategy=strategy)\n",
    "\n",
    "    # Oversample the minority classes to balance the dataset\n",
    "    X_data, y_data = oversample.fit_resample(X_data, y_data)\n",
    "\n",
    "    # Return the balanced features and target variable\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_column_names(data):\n",
    "    \"\"\"\n",
    "    Fix column names in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with potentially incorrect column names.\n",
    "\n",
    "    Returns:\n",
    "    - Fixed DataFrame with corrected column names.\n",
    "\n",
    "    Steps:\n",
    "    1. Rename columns to correct any inconsistencies.\n",
    "    2. Return the DataFrame with updated column names.\n",
    "\n",
    "    \"\"\"\n",
    "    # Rename columns to correct any inconsistencies\n",
    "    data.rename(columns={'img_red_mean_date5':'img_red_mean_date0',\n",
    "                         'img_green_mean_date5':'img_green_mean_date0',\n",
    "                         'img_blue_mean_date5':'img_blue_mean_date0',\n",
    "                         'img_red_std_date5':'img_red_std_date0',\n",
    "                         'img_green_std_date5':'img_green_std_date0',\n",
    "                         'img_blue_std_date5':'img_blue_std_date0'}, inplace=True)\n",
    "\n",
    "    # Return the DataFrame with updated column names\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(data_enumerate, change_status_columns, date_cols):\n",
    "    \"\"\"\n",
    "    Replace categorical values with their respective enumeration.\n",
    "\n",
    "    Parameters:\n",
    "    - data_enumerate: DataFrame containing categorical columns to be enumerated.\n",
    "    - change_status_columns: List of column names containing change status.\n",
    "    - date_cols: List of column names containing dates.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with categorical values replaced by their enumeration.\n",
    "    - List of unique dates.\n",
    "    - List of unique change statuses.\n",
    "\n",
    "    Steps:\n",
    "    1. Get unique dates and change statuses.\n",
    "    2. Create dictionaries to map unique values to their enumeration.\n",
    "    3. Replace categorical values with their respective enumeration.\n",
    "    4. Return the DataFrame with replaced values, along with lists of unique dates and change statuses.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get unique dates and change types\n",
    "    unique_dates = sorted(data_enumerate[date_cols].stack().unique().tolist())\n",
    "    unique_change_status = data_enumerate[change_status_columns].stack().unique().tolist()\n",
    "\n",
    "    # Create dictionaries with enumeration\n",
    "    dict_dates = {date: index for index, date in enumerate(unique_dates)}\n",
    "    dict_changes = {change: index for index, change in enumerate(unique_change_status)}\n",
    "\n",
    "    # Replace categorical values with their enumeration\n",
    "    data_enumerate[date_cols] = data_enumerate[date_cols].replace(dict_dates)\n",
    "    data_enumerate[change_status_columns] = data_enumerate[change_status_columns].replace(dict_changes)\n",
    "\n",
    "    # Return the DataFrame with replaced values, along with lists of unique dates and change statuses\n",
    "    return data_enumerate, unique_dates, unique_change_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dates_values(data_sorted, unique_dates, unique_change_status, date_cols, change_status_columns, value_columns):\n",
    "    \"\"\"\n",
    "    Sort the date values and corresponding columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data_sorted: DataFrame to be sorted.\n",
    "    - unique_dates: List of unique dates.\n",
    "    - unique_change_status: List of unique change statuses.\n",
    "    - date_cols: List of column names containing dates.\n",
    "    - change_status_columns: List of column names containing change status.\n",
    "    - value_columns: List of column names containing values.\n",
    "\n",
    "    Returns:\n",
    "    - Sorted DataFrame with dates and values.\n",
    "\n",
    "    Steps:\n",
    "    1. Reset the index of the DataFrame.\n",
    "    2. Get a mask where each element is True if all date columns in the row are monotonically increasing.\n",
    "    3. Get the indices where any of the rows do not have monotonically increasing dates.\n",
    "    4. Iterate over non-monotonic indices.\n",
    "        - Sort dates and values together.\n",
    "        - Update the DataFrame with sorted values and dates.\n",
    "    5. Create dictionaries with enumeration.\n",
    "    6. Replace categorical values with their respective enumeration.\n",
    "    7. Return the sorted DataFrame.\n",
    "\n",
    "    \"\"\"\n",
    "    # Reset the index of the DataFrame\n",
    "    data_sorted = data_sorted.reset_index(drop=True)\n",
    "\n",
    "    # Get the mask where each element is True if all date columns in the row are monotonically increasing\n",
    "    monotonic_mask = data_sorted[date_cols].apply(lambda row: row.is_monotonic_increasing, axis=1)\n",
    "\n",
    "    # Get the indices where any of the rows do not have monotonically increasing dates\n",
    "    non_monotonic_indices = data_sorted.index[~monotonic_mask].tolist()\n",
    "\n",
    "    # Iterate over non-monotonic indices\n",
    "    for index in non_monotonic_indices:\n",
    "        row = data_sorted.loc[index]\n",
    "        if index % 50000 == 0:\n",
    "            print(index)\n",
    "        dates = row[date_cols]\n",
    "\n",
    "        img_values = [row[value_columns].values[i:i + 6] for i in range(0, len(row[value_columns].values), 6)]\n",
    "        status_values = row[change_status_columns]\n",
    "\n",
    "        # Sort dates and values together\n",
    "        sorted_indices = np.argsort(dates)\n",
    "        sorted_dates = np.array(dates)[sorted_indices]\n",
    "        sorted_values = np.array(img_values)[sorted_indices]\n",
    "        sorted_status = np.array(status_values)[sorted_indices]\n",
    "\n",
    "        # Update the DataFrame with sorted values and dates\n",
    "        data_sorted.loc[index, date_cols] = sorted_dates\n",
    "        data_sorted.loc[index, value_columns] = np.reshape(sorted_values[:, :6], 30)\n",
    "        data_sorted.loc[index, change_status_columns] = sorted_status\n",
    "\n",
    "    # Create dictionaries with enumeration\n",
    "    dict_dates = {index: date for index, date in enumerate(unique_dates)}\n",
    "    dict_changes = {index: change for index, change in enumerate(unique_change_status)}\n",
    "\n",
    "    # Replace categorical values with their respective enumeration\n",
    "    data_sorted[date_cols] = data_sorted[date_cols].replace(dict_dates)\n",
    "    data_sorted[change_status_columns] = data_sorted[change_status_columns].replace(dict_changes)\n",
    "\n",
    "    # Return the sorted DataFrame\n",
    "    return data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date_columns(data, change_status_columns, name=''):\n",
    "    \"\"\"\n",
    "    Fix missing values in date columns and convert them to datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "    - change_status_columns: List of column names containing change status.\n",
    "    - name: Name to be appended to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with fixed date columns.\n",
    "\n",
    "    Steps:\n",
    "    1. Define the date columns.\n",
    "    2. Fill missing values with NaT, convert the DataFrame to object type, and fill remaining NaN values.\n",
    "    3. Use SimpleImputer to fill missing values in date columns with the most frequent value.\n",
    "    4. Convert date columns to datetime format.\n",
    "    5. Extract unique dates and change types.\n",
    "    6. Generate value column names.\n",
    "    7. Sort dates and values together.\n",
    "    8. Save the DataFrame to a CSV file.\n",
    "    9. Return the DataFrame with fixed date columns.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the date columns\n",
    "    date_cols = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "\n",
    "    # Fill missing values with NaT, convert to object type, and fill remaining NaN values\n",
    "    data = data.fillna(pd.NaT).astype(object).fillna(np.nan)\n",
    "\n",
    "    # Use SimpleImputer to fill missing values in date columns with the most frequent value\n",
    "    date_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[date_cols] = date_imputer.fit_transform(data[date_cols])\n",
    "\n",
    "    # Convert date columns to datetime format\n",
    "    data.loc[:, date_cols] = data.loc[:, date_cols].apply(pd.to_datetime, format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Extract unique dates and change types\n",
    "    data, unique_dates, unique_change_types = unique_values(data, change_status_columns, date_cols)\n",
    "\n",
    "    # Generate value column names\n",
    "    value_columns = np.array([[f'img_red_mean_date{i}',\n",
    "                               f'img_green_mean_date{i}',\n",
    "                               f'img_blue_mean_date{i}',\n",
    "                               f'img_red_std_date{i}',\n",
    "                               f'img_green_std_date{i}',\n",
    "                               f'img_blue_std_date{i}'] for i in range(0, 5)]).reshape(30)\n",
    "\n",
    "    # Sort dates and values together\n",
    "    data_fixed_type = sort_dates_values(data, unique_dates, unique_change_types, date_cols, change_status_columns,\n",
    "                                        value_columns)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_fixed_type.to_csv(f'../data/data_fixed_type{name}.csv')\n",
    "\n",
    "    # Return the DataFrame with fixed date columns\n",
    "    return data_fixed_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_change_status(data, change_status_columns):\n",
    "    \"\"\"\n",
    "    Replace change status strings with numerical values.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "    - change_status_columns: List of column names containing change status.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with change status replaced by numerical values.\n",
    "\n",
    "    Steps:\n",
    "    1. Define a dictionary to map change status strings to numerical values.\n",
    "    2. Replace change status strings with numerical values.\n",
    "    3. Return the DataFrame with fixed change status.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define a dictionary to map change status strings to numerical values\n",
    "    dict_change_status = {\n",
    "        'Greenland': 0, 'Prior Construction': 1, 'Land Cleared': 2, 'Materials Dumped': 3, 'Materials Introduced': 4,\n",
    "        'Excavation': 5, 'Construction Started': 6, 'Construction Midway': 7, 'Construction Done': 8, 'Operational': 9\n",
    "    }\n",
    "\n",
    "    # Replace change status strings with numerical values\n",
    "    data.loc[:, change_status_columns] = data.loc[:, change_status_columns].replace(dict_change_status)\n",
    "\n",
    "    # Return the DataFrame with fixed change status\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumb_hot_encoder(df, col):\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on a column containing comma-separated values.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - col: Name of the column to encode.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with one-hot encoded columns.\n",
    "\n",
    "    Steps:\n",
    "    1. Split the column values by comma and remove rows with ['N', 'A'] values.\n",
    "    2. Perform one-hot encoding on the split values.\n",
    "    3. Replace True/False with 1/0.\n",
    "    4. Drop the 'nan' column if it exists.\n",
    "\n",
    "    \"\"\"\n",
    "    # Preprocess the column\n",
    "    df_col_nan = df[df[col].isna()].drop(columns=col)\n",
    "    df_col_not_nan = df[~df[col].isna()]\n",
    "    treated_col_type = df_col_not_nan[col].astype(str).str.split(',')\n",
    "    treated_col_type = treated_col_type[~treated_col_type.apply(lambda x: x == ['N', 'A'])]\n",
    "    label_list = treated_col_type.explode()\n",
    "\n",
    "    # Perform one-hot encoding\n",
    "    one_hot_encoded = pd.get_dummies(label_list, prefix=col, dummy_na=True).groupby(level=0).max().astype(int)\n",
    "    df_col_not_nan.loc[:, one_hot_encoded.columns] = one_hot_encoded\n",
    "    df_col_not_nan.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # Replace True/False with 1/0\n",
    "    df_col_not_nan.replace({True: 1, False: 0}, inplace=True)\n",
    "\n",
    "    # Drop 'nan' column if exists\n",
    "    df_col_not_nan.drop(columns=np.nan, errors='ignore', inplace=True)\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    df = pd.concat([df_col_not_nan, df_col_nan])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_train_data(data_filtered):\n",
    "    \"\"\"\n",
    "    Filter the training data to remove rows with excessive NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    - data_filtered: DataFrame containing the training data.\n",
    "\n",
    "    Returns:\n",
    "    - Filtered DataFrame.\n",
    "\n",
    "    Steps:\n",
    "    1. Check for NaN values in the DataFrame.\n",
    "    2. Count the number of NaN values in each row.\n",
    "    3. Select rows with fewer than 5 NaN values and where at least one of the specified date columns is not NaN.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check for NaN values\n",
    "    nan_values = data_filtered.isna()\n",
    "\n",
    "    # Count the number of NaN values in each row\n",
    "    nan_count_per_row = nan_values.sum(axis=1)\n",
    "\n",
    "    # Select rows with fewer than 5 NaN values and where at least one of the specified date columns is not NaN\n",
    "    index_maintain = data_filtered[(nan_count_per_row < 5) & (~data_filtered[['date0', 'date1', 'date2', 'date3', 'date4']].isna().any(axis=1))].index\n",
    "\n",
    "    # Filter rows\n",
    "    data_filtered = data_filtered.iloc[index_maintain, :]\n",
    "\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(data_change_img, date_cols):\n",
    "    \"\"\"\n",
    "    Add date-related features to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data_change_img: DataFrame containing the data.\n",
    "    - date_cols: List of column names containing date information.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with added date features.\n",
    "\n",
    "    Steps:\n",
    "    1. Convert date columns to datetime format.\n",
    "    2. Calculate time differences between consecutive date columns.\n",
    "    3. Iterate over date ranges and color-statistic combinations to calculate change ratios.\n",
    "    4. Calculate change in status between consecutive dates.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert date columns to datetime format\n",
    "        data_change_img.loc[:, date_cols] = data_change_img.loc[:, date_cols].apply(pd.to_datetime, format=\"%Y-%m-%d\")\n",
    "    except:\n",
    "        data_change_img.loc[:, date_cols] = data_change_img.loc[:, date_cols].apply(pd.to_datetime, format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Calculate time differences between consecutive date columns\n",
    "    for date in range(0, 4):\n",
    "        data_change_img[f'diff_date_{date + 1}_{date}'] = (data_change_img[f'date{date + 1}'] - data_change_img[f'date{date}']).apply(lambda x: int(str(x).split(\" \")[0]))\n",
    "\n",
    "    # Define lists for image colors and statistical values\n",
    "    list_colors = ['green', 'blue', 'red']\n",
    "    list_values = ['mean', 'std']\n",
    "\n",
    "    # Iterate over date ranges and color-statistic combinations\n",
    "    for date in range(0, 4):\n",
    "        for color in list_colors:\n",
    "            for value in list_values:\n",
    "                col1 = f'img_{color}_{value}_date{date}'\n",
    "                col2 = f'img_{color}_{value}_date{date + 1}'\n",
    "\n",
    "                # Calculate change ratios for consecutive dates\n",
    "                data_change_img[f'img_{color}_{value}_change_{date + 1}_{date}'] = (data_change_img[col2] - data_change_img[col1]) / data_change_img[f'diff_date_{date + 1}_{date}']\n",
    "\n",
    "        col3 = f'change_status_date{date}'\n",
    "        col4 = f'change_status_date{date + 1}'\n",
    "        # Calculate change in status between consecutive dates\n",
    "        data_change_img[f'change_status_date_{date + 1}_{date}'] = data_change_img[col4] - data_change_img[col3]\n",
    "\n",
    "    return data_change_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_important_features(data):\n",
    "    \"\"\"\n",
    "    Add important features to the DataFrame using different strategies.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with added important features.\n",
    "\n",
    "    Strategies:\n",
    "    1. Interaction terms: Sum of important features.\n",
    "    2. Polynomial features: Square of each important feature.\n",
    "    3. Transformations: Square root of absolute values of important features.\n",
    "    4. Statistical aggregations: Mean and standard deviation of important features.\n",
    "\n",
    "    \"\"\"\n",
    "    important_features = ['perimeter', 'centroid_x', 'centroid_y', 'boundary_length', 'change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3', 'change_status_date4', 'diff_date_1_0', 'diff_date_2_1', 'diff_date_3_2', 'diff_date_4_3', 'img_blue_mean_date0', 'img_blue_mean_date1', 'img_blue_mean_date2', 'img_blue_mean_date3', 'img_blue_mean_date4']\n",
    "    \n",
    "    # Strategy 1: Interaction terms\n",
    "    data['interaction_feature'] = data[important_features].sum(axis=1)\n",
    "\n",
    "    # Strategy 2: Polynomial features\n",
    "    for feature in important_features:\n",
    "        data[f'{feature}_squared'] = data[feature] ** 2\n",
    "\n",
    "    # Strategy 3: Transformations\n",
    "    for feature in important_features:\n",
    "        data[f'sqrt_{feature}'] = np.sqrt(abs(data[feature]))\n",
    "\n",
    "    # Strategy 4: Statistical aggregations\n",
    "    data['mean_feature'] = data[important_features].mean(axis=1)\n",
    "    data['std_feature'] = data[important_features].std(axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_1(data):\n",
    "    \"\"\"\n",
    "    Normalize the data using standardization.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with normalized data.\n",
    "\n",
    "    \"\"\"\n",
    "    data_norm = data.copy()\n",
    "    # standarization\n",
    "    v = np.array([[f'img_red_mean_date{i}',\n",
    "                    f'img_green_mean_date{i}',\n",
    "                    f'img_blue_mean_date{i}',\n",
    "                    f'img_red_std_date{i}',\n",
    "                    f'img_green_std_date{i}',\n",
    "                    f'img_blue_std_date{i}'] for i in range(0,5)]).reshape(30)\n",
    "    geo = np.array(['perimeter', 'area', 'centroid_x', 'centroid_y'])\n",
    "    new_features = ['interaction_feature','perimeter_squared','centroid_x_squared','centroid_y_squared','boundary_length_squared','change_status_date0_squared',\n",
    "                    'change_status_date1_squared','change_status_date2_squared','change_status_date3_squared','change_status_date4_squared','diff_date_1_0_squared',\n",
    "                    'diff_date_2_1_squared','diff_date_3_2_squared','diff_date_4_3_squared','img_blue_mean_date0_squared','img_blue_mean_date1_squared','img_blue_mean_date2_squared',\n",
    "                    'img_blue_mean_date3_squared','img_blue_mean_date4_squared','sqrt_perimeter','sqrt_centroid_x','sqrt_centroid_y','sqrt_boundary_length','sqrt_change_status_date0',\n",
    "                    'sqrt_change_status_date1','sqrt_change_status_date2','sqrt_change_status_date3','sqrt_change_status_date4','sqrt_diff_date_1_0','sqrt_diff_date_2_1','sqrt_diff_date_3_2',\n",
    "                    'sqrt_diff_date_4_3','sqrt_img_blue_mean_date0','sqrt_img_blue_mean_date1','sqrt_img_blue_mean_date2','sqrt_img_blue_mean_date3','sqrt_img_blue_mean_date4','mean_feature','std_feature']\n",
    "    v_geo = np.concatenate([v, geo, new_features])\n",
    "    data_norm[v_geo] = (data_norm[v_geo] - data_norm[v_geo].mean())/data_norm[v_geo].std()\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_2(data):\n",
    "    \"\"\"\n",
    "    Normalize the data using standardization.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with normalized data.\n",
    "\n",
    "    \"\"\"\n",
    "    data_norm = data.copy()\n",
    "    # standarization\n",
    "    dont_normalize = ['change_status_date0','change_status_date1','change_status_date2','change_status_date3','change_status_date4','urban_type_Dense Urban', 'urban_type_Industrial', 'urban_type_Rural',\n",
    "       'urban_type_Sparse Urban', 'urban_type_Urban Slum','geography_type_Barren Land', 'geography_type_Coastal','geography_type_Dense Forest', 'geography_type_Desert',\n",
    "       'geography_type_Farms', 'geography_type_Grass Land','geography_type_Hills', 'geography_type_Lakes', 'geography_type_River','geography_type_Snow', \n",
    "       'geography_type_Sparse Forest']\n",
    "    all_columns = list(data_norm.columns)\n",
    "    to_normalize = [x for x in all_columns if x not in dont_normalize]\n",
    "    data_norm[to_normalize] = (data_norm[to_normalize] - data_norm[to_normalize].mean())/data_norm[to_normalize].std()\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_3(data):\n",
    "    \"\"\"\n",
    "    Normalize the data using standardization.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with normalized data.\n",
    "\n",
    "    \"\"\"\n",
    "    data_norm = data.copy()\n",
    "    all_columns = list(data_norm.columns)\n",
    "    # to_normalize = [x for x in all_columns if not any(substring in x for substring in dont_normalize)]\n",
    "    to_normalize = all_columns\n",
    "    data_norm[to_normalize] = (data_norm[to_normalize] - data_norm[to_normalize].mean()) / data_norm[to_normalize].std()\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training(train_dataset):\n",
    "    \"\"\"\n",
    "    Preprocess the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dataset: DataFrame containing the training dataset.\n",
    "\n",
    "    Returns:\n",
    "    - data_norm: DataFrame with preprocessed and normalized data.\n",
    "    - y_data: Target variable.\n",
    "    - data: DataFrame with preprocessed data.\n",
    "\n",
    "    Steps:\n",
    "    1. Replace 'N,A', None, 'nan', np.inf, and -np.inf with NaN.\n",
    "    2. Drop rows with more than 4 NaN values and no missing date columns.\n",
    "    3. Add date-related features.\n",
    "    4. Map 'change_type' values to integers.\n",
    "    5. Encode categorical columns using one-hot encoding.\n",
    "    6. Drop unnecessary columns.\n",
    "    7. Impute missing numerical values.\n",
    "    8. Create synthetic samples using SMOTE.\n",
    "    9. Add important features.\n",
    "    10. Normalize the data.\n",
    "\n",
    "    \"\"\"\n",
    "    data = train_dataset.copy().reset_index(drop=True)\n",
    "    data = data.replace(['N,A', None, 'nan', np.inf, -np.inf], np.nan)\n",
    "\n",
    "    change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4, 'Mega Projects': 5}\n",
    "\n",
    "    date_columns = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "\n",
    "    nan_values = data.isna()\n",
    "    nan_count_per_row = nan_values.sum(axis=1)\n",
    "\n",
    "    index_maintain = data[(nan_count_per_row < 5) & (~data[date_columns].isna().any(axis=1))].index \n",
    "    data = data.iloc[index_maintain, :]\n",
    "\n",
    "    data = add_date_features(data, date_columns)\n",
    "\n",
    "    data['change_type'] = data['change_type'].apply(lambda x: change_type_map[x])\n",
    "\n",
    "    columns_to_encode = ['urban_type', 'geography_type']\n",
    "    for column in columns_to_encode:\n",
    "        data = dumb_hot_encoder(data, column)\n",
    "    data.drop(columns=['urban_type_nan', 'geography_type_nan'], inplace=True)\n",
    "\n",
    "    try:\n",
    "        data.drop(columns='Unnamed: 0', inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    data.drop(columns=date_columns, inplace=True)\n",
    "    data = numerical_imputer(data)\n",
    "\n",
    "    data, y_data = create_data(data)\n",
    "\n",
    "    data = add_important_features(data)\n",
    "\n",
    "    data_norm = data.copy()\n",
    "    data_norm = normalize_data_2(data_norm)\n",
    "\n",
    "    return data_norm, y_data, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_test(test_dataset):\n",
    "    \"\"\"\n",
    "    Preprocess the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - test_dataset: DataFrame containing the test dataset.\n",
    "\n",
    "    Returns:\n",
    "    - data_norm: DataFrame with preprocessed and normalized data.\n",
    "    - data: DataFrame with preprocessed data.\n",
    "\n",
    "    Steps:\n",
    "    1. Replace 'N,A', None, 'nan', np.inf, and -np.inf with NaN.\n",
    "    2. Fill missing values in date columns with the most frequent date.\n",
    "    3. Fill missing values in change status columns with the median.\n",
    "    4. Add date-related features.\n",
    "    5. Encode categorical columns using one-hot encoding.\n",
    "    6. Drop unnecessary columns.\n",
    "    7. Impute missing numerical values.\n",
    "    8. Add important features.\n",
    "    9. Normalize the data.\n",
    "\n",
    "    \"\"\"\n",
    "    data = test_dataset.copy().reset_index(drop=True)\n",
    "    data = data.replace(['N,A', None, 'nan', np.inf, -np.inf], np.nan)\n",
    "\n",
    "    date_columns = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "\n",
    "    data = data.fillna(pd.NaT).astype(object).fillna(np.nan)\n",
    "\n",
    "    date_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[date_columns] = date_imputer.fit_transform(data[date_columns])\n",
    "\n",
    "    status_imputer = SimpleImputer(strategy='median')\n",
    "    change_status_columns = [f'change_status_date{i}' for i in range(0, 5)]\n",
    "    data[change_status_columns] = status_imputer.fit_transform(data[change_status_columns])\n",
    "\n",
    "    data = add_date_features(data, date_columns)\n",
    "\n",
    "    columns_to_encode = ['urban_type', 'geography_type']\n",
    "    for column in columns_to_encode:\n",
    "        data = dumb_hot_encoder(data, column)\n",
    "    data.drop(columns=['urban_type_nan', 'geography_type_nan'], inplace=True)\n",
    "\n",
    "    try:\n",
    "        data.drop(columns='Unnamed: 0', inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    data.drop(columns=date_columns, inplace=True)\n",
    "    data = numerical_imputer(data)    \n",
    "    data = add_important_features(data)\n",
    "    data_norm = data.copy()\n",
    "    data_norm = normalize_data_2(data_norm)\n",
    "\n",
    "    return data_norm, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_important_features(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Find important features using a Random Forest classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - X_data: DataFrame containing the features.\n",
    "    - y_data: Series containing the target labels.\n",
    "\n",
    "    Returns:\n",
    "    - selected_feature_names: List of selected feature names.\n",
    "\n",
    "    Steps:\n",
    "    1. Initialize a Random Forest classifier.\n",
    "    2. Initialize SelectFromModel with the Random Forest classifier and the desired threshold or number of features.\n",
    "    3. Fit SelectFromModel to the data and transform the data.\n",
    "    4. Get the indices of selected features.\n",
    "    5. Get the names of selected features.\n",
    "    6. Print the selected feature names.\n",
    "    7. Return the list of selected feature names.\n",
    "\n",
    "    \"\"\"\n",
    "    rf_classifier = rf()\n",
    "\n",
    "    sfm = SelectFromModel(estimator=rf_classifier, threshold='median')\n",
    "\n",
    "    X_selected = sfm.fit_transform(X_data, y_data)\n",
    "\n",
    "    selected_mask = sfm.get_support()\n",
    "\n",
    "    selected_indices = [i for i, selected in enumerate(selected_mask) if selected]\n",
    "\n",
    "    selected_feature_names = [X_data.columns[i] for i in selected_indices]\n",
    "    print(\"Selected features:\", selected_feature_names)\n",
    "    return selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, type='Training'):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained classifier model.\n",
    "    - X: DataFrame containing the features.\n",
    "    - y: Series containing the target labels.\n",
    "    - type: Type of evaluation (e.g., 'Training', 'Validation', 'Test').\n",
    "\n",
    "    Steps:\n",
    "    1. Print the evaluation type.\n",
    "    2. Predict the labels using the model.\n",
    "    3. Calculate F1 scores (macro, micro, weighted) and accuracy score.\n",
    "    4. Print the F1 scores and accuracy score.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"==================={type}===============\")\n",
    "    pred_y = model.predict(X)\n",
    "    score_macro = f1_score(y, pred_y, average='macro')\n",
    "    score_micro = f1_score(y, pred_y, average='micro')\n",
    "    score_weighted = f1_score(y, pred_y, average='weighted')\n",
    "    score = model.score(X, y)\n",
    "    print(f\"-> score macro: {score_macro}\")\n",
    "    print(\"----------------------------\")\n",
    "    print(f\"-> score micro: {score_micro}\")\n",
    "    print(f\"-> score weighted: {score_weighted}\")\n",
    "    print(f\"-> score      : {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_original = gpd.read_file('../data/train.geojson', index_col=0)\n",
    "test_df_original = gpd.read_file('../data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_original.copy()\n",
    "test_df = test_df_original.copy()\n",
    "\n",
    "train_df = train_df.replace({'N,A':np.nan, None: np.nan})\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "data = prepare_data(train_df)\n",
    "test_df = prepare_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = filter_train_data(data)\n",
    "data_fix_columns = fix_column_names(data_filtered)\n",
    "data_fix_columns_test = fix_column_names(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_status_columns = [f'change_status_date{i}' for i in range(0, 5)]\n",
    "data_fix_status = fix_change_status(data_fix_columns,change_status_columns)\n",
    "data_fix_status_test = fix_change_status(data_fix_columns_test,change_status_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fix_date = fix_date_columns(data_fix_status,change_status_columns)\n",
    "data_fix_date_test = fix_date_columns(data_fix_status_test,change_status_columns,'_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data with fixed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/data_fixed_type.csv', index_col=0)\n",
    "test_df = pd.read_csv('../data/data_fixed_type_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_not_nan.drop(columns=[col], inplace=True)\n",
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_col_not_nan.replace({True: 1, False: 0}, inplace=True)\n",
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_not_nan.replace({True: 1, False: 0}, inplace=True)\n",
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_not_nan.drop(columns=np.nan, errors='ignore', inplace=True)\n",
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_not_nan.drop(columns=[col], inplace=True)\n",
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_col_not_nan.replace({True: 1, False: 0}, inplace=True)\n",
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_not_nan.replace({True: 1, False: 0}, inplace=True)\n",
      "/var/folders/qk/g2d67vns3fq15rbqpxmhdrqm0000gn/T/ipykernel_15796/817560699.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_not_nan.drop(columns=np.nan, errors='ignore', inplace=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_data, y_data, X_data_pre_norm \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m X_submit, X_submit_pre_norm \u001b[38;5;241m=\u001b[39m preprocessing_test(test_df)\n",
      "Cell \u001b[0;32mIn[22], line 54\u001b[0m, in \u001b[0;36mpreprocess_training\u001b[0;34m(train_dataset)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     53\u001b[0m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mdate_columns, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_imputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m data, y_data \u001b[38;5;241m=\u001b[39m create_data(data)\n\u001b[1;32m     58\u001b[0m data \u001b[38;5;241m=\u001b[39m add_important_features(data)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mnumerical_imputer\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# imputer = IterativeImputer()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m imputer\u001b[38;5;241m.\u001b[39mfit(dataframe[dataframe[numerical_columns]\u001b[38;5;241m.\u001b[39mnotna()\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m---> 24\u001b[0m dataframe[dataframe[numerical_columns]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumerical_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataframe\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_knn.py:366\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[1;32m    358\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[1;32m    359\u001b[0m     X[row_missing_idx, :],\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[1;32m    365\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# process_chunk modifies X in place. No return value.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/pairwise.py:2153\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2152\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2153\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2154\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_knn.py:349\u001b[0m, in \u001b[0;36mKNNImputer.transform.<locals>.process_chunk\u001b[0;34m(dist_chunk, start)\u001b[0m\n\u001b[1;32m    344\u001b[0m     dist_subset \u001b[38;5;241m=\u001b[39m dist_chunk[dist_idx_map[receivers_idx] \u001b[38;5;241m-\u001b[39m start][\n\u001b[1;32m    345\u001b[0m         :, potential_donors_idx\n\u001b[1;32m    346\u001b[0m     ]\n\u001b[1;32m    348\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neighbors, \u001b[38;5;28mlen\u001b[39m(potential_donors_idx))\n\u001b[0;32m--> 349\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_impute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m X[receivers_idx, col] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_knn.py:184\u001b[0m, in \u001b[0;36mKNNImputer._calc_impute\u001b[0;34m(self, dist_pot_donors, n_neighbors, fit_X_col, mask_fit_X_col)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to impute a single column.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Imputed values for receiver.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Get donors\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m donors_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_pot_donors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    185\u001b[0m     :, :n_neighbors\n\u001b[1;32m    186\u001b[0m ]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Get weight matrix from distance matrix\u001b[39;00m\n\u001b[1;32m    189\u001b[0m donors_dist \u001b[38;5;241m=\u001b[39m dist_pot_donors[\n\u001b[1;32m    190\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(donors_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m], donors_idx\n\u001b[1;32m    191\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:858\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_data, y_data, X_data_pre_norm = preprocess_training(train_df)\n",
    "X_submit, X_submit_pre_norm = preprocessing_test(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_pre_norm = pd.read_csv('X_data_pre_norm_2.csv')\n",
    "X_submit_pre_norm = pd.read_csv('X_submit_pre_norm_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = normalize_data_2(X_data_pre_norm)\n",
    "X_submit = normalize_data_2(X_submit_pre_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.to_csv('X_data_2.csv')\n",
    "X_submit.to_csv('X_submit_2.csv')\n",
    "y_data.to_csv('y_data_2.csv')\n",
    "X_data_pre_norm.to_csv('X_data_pre_norm_2.csv')\n",
    "X_submit_pre_norm.to_csv('X_submit_pre_norm_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = find_important_features(X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_data = X_data.reset_index(drop=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------RandomForest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----------------RandomForest')\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest_model_2.pkl', 'wb') as file:\n",
    "    pkl.dump(rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf, X_train, y_train)\n",
    "\n",
    "evaluate_model(rf, X_test, y_test, type='Test')\n",
    "\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change order of columns\n",
    "X_submission = X_submit[X_data.columns]\n",
    "# X_submission = X_submission\n",
    "\n",
    "pred_y = rf.predict(X_submission)\n",
    "pred_df = pd.DataFrame(pred_y, columns=['change_type'])\n",
    "pred_df.to_csv(\"new_features_submission.csv\", index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
